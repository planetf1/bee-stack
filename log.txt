
==> Audit <==
|--------------|--------------------------------|----------|--------|---------|---------------------|---------------------|
|   Command    |              Args              | Profile  |  User  | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| start        | --profile minikube --driver    | minikube | jonesn | v1.34.0 | 04 Nov 24 14:56 GMT | 04 Nov 24 14:57 GMT |
|              | podman --container-runtime     |          |        |         |                     |                     |
|              | cri-o                          |          |        |         |                     |                     |
| help         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:05 GMT | 04 Nov 24 15:05 GMT |
| config       | help                           | minikube | jonesn | v1.34.0 | 04 Nov 24 15:09 GMT | 04 Nov 24 15:09 GMT |
| config       | help get insecure-registry     | minikube | jonesn | v1.34.0 | 04 Nov 24 15:09 GMT | 04 Nov 24 15:09 GMT |
| config       | get insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:09 GMT |                     |
| config       | get insecure-registry =        | minikube | jonesn | v1.34.0 | 04 Nov 24 15:10 GMT |                     |
|              | ["docker.io", "quay.io"]       |          |        |         |                     |                     |
| config       | set insecure-registry =        | minikube | jonesn | v1.34.0 | 04 Nov 24 15:10 GMT |                     |
|              | ["docker.io", "quay.io"]       |          |        |         |                     |                     |
| config       | set insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:10 GMT | 04 Nov 24 15:10 GMT |
|              | ["docker.io", "quay.io"]       |          |        |         |                     |                     |
| config       | get insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:10 GMT | 04 Nov 24 15:10 GMT |
| config       | get insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:23 GMT | 04 Nov 24 15:23 GMT |
| config       | set insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:24 GMT | 04 Nov 24 15:24 GMT |
|              | docker.io,quay.io              |          |        |         |                     |                     |
| config       | set insecure-registry          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:27 GMT | 04 Nov 24 15:27 GMT |
|              | docker.io                      |          |        |         |                     |                     |
| stop         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:27 GMT | 04 Nov 24 15:28 GMT |
| start        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:28 GMT | 04 Nov 24 15:28 GMT |
| stop         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:28 GMT | 04 Nov 24 15:28 GMT |
| start        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:28 GMT | 04 Nov 24 15:29 GMT |
| stop         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:29 GMT | 04 Nov 24 15:29 GMT |
| start        | --insecure-registry=docker.io  | minikube | jonesn | v1.34.0 | 04 Nov 24 15:29 GMT | 04 Nov 24 15:29 GMT |
| addons       | configure registry-creds       | minikube | jonesn | v1.34.0 | 04 Nov 24 15:30 GMT | 04 Nov 24 15:30 GMT |
| addons       | configure registry-creds       | minikube | jonesn | v1.34.0 | 04 Nov 24 15:33 GMT |                     |
| addons       | list                           | minikube | jonesn | v1.34.0 | 04 Nov 24 15:33 GMT | 04 Nov 24 15:33 GMT |
| addons       | install auto-pause             | minikube | jonesn | v1.34.0 | 04 Nov 24 15:34 GMT | 04 Nov 24 15:34 GMT |
| addons       | enable auto-pause              | minikube | jonesn | v1.34.0 | 04 Nov 24 15:35 GMT | 04 Nov 24 15:35 GMT |
| addons       | enable registry                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:35 GMT |                     |
| addons       | disable registry               | minikube | jonesn | v1.34.0 | 04 Nov 24 15:36 GMT |                     |
| addons       | enable metrics-server          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:36 GMT |                     |
| unpause      |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:37 GMT | 04 Nov 24 15:37 GMT |
| addons       | disable registry               | minikube | jonesn | v1.34.0 | 04 Nov 24 15:37 GMT | 04 Nov 24 15:37 GMT |
| addons       | enable metrics-server          | minikube | jonesn | v1.34.0 | 04 Nov 24 15:37 GMT | 04 Nov 24 15:37 GMT |
| unpause      |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:38 GMT | 04 Nov 24 15:38 GMT |
| addons       | configure auto-pause           | minikube | jonesn | v1.34.0 | 04 Nov 24 15:38 GMT | 04 Nov 24 15:38 GMT |
| pause        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:38 GMT | 04 Nov 24 15:39 GMT |
| unpause      |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:39 GMT | 04 Nov 24 15:39 GMT |
| stop         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:40 GMT | 04 Nov 24 15:40 GMT |
| start        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:40 GMT | 04 Nov 24 15:41 GMT |
| addons       | disable metrics-server         | minikube | jonesn | v1.34.0 | 04 Nov 24 15:42 GMT | 04 Nov 24 15:42 GMT |
| stop         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:43 GMT | 04 Nov 24 15:43 GMT |
| start        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:43 GMT | 04 Nov 24 15:43 GMT |
| help         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 15:44 GMT | 04 Nov 24 15:44 GMT |
| update-check |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 16:54 GMT | 04 Nov 24 16:54 GMT |
| help         |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 17:18 GMT | 04 Nov 24 17:18 GMT |
| delete       |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 17:18 GMT | 04 Nov 24 17:18 GMT |
| start        | --profile minikube --driver    | minikube | jonesn | v1.34.0 | 04 Nov 24 17:18 GMT | 04 Nov 24 17:19 GMT |
|              | podman --container-runtime     |          |        |         |                     |                     |
|              | cri-o                          |          |        |         |                     |                     |
| addons       | enable auto-pause              | minikube | jonesn | v1.34.0 | 04 Nov 24 17:19 GMT | 04 Nov 24 17:19 GMT |
| addons       | configure auto-pause           | minikube | jonesn | v1.34.0 | 04 Nov 24 17:19 GMT | 04 Nov 24 17:19 GMT |
| addons       | configure auto-pause           | minikube | jonesn | v1.34.0 | 04 Nov 24 17:20 GMT |                     |
| delete       |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 17:21 GMT | 04 Nov 24 17:21 GMT |
| start        | --profile minikube --driver    | minikube | jonesn | v1.34.0 | 04 Nov 24 17:21 GMT | 04 Nov 24 17:22 GMT |
|              | podman --container-runtime     |          |        |         |                     |                     |
|              | cri-o                          |          |        |         |                     |                     |
| start        |                                | minikube | jonesn | v1.34.0 | 04 Nov 24 17:23 GMT | 04 Nov 24 17:23 GMT |
| addons       | enable auto-pause              | minikube | jonesn | v1.34.0 | 04 Nov 24 17:23 GMT | 04 Nov 24 17:23 GMT |
|--------------|--------------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/11/04 17:23:02
Running on machine: NigelMacBook
Binary: Built with gc go1.22.5 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1104 17:23:02.716344   50934 out.go:345] Setting OutFile to fd 1 ...
I1104 17:23:02.717187   50934 out.go:397] isatty.IsTerminal(1) = true
I1104 17:23:02.717189   50934 out.go:358] Setting ErrFile to fd 2...
I1104 17:23:02.717192   50934 out.go:397] isatty.IsTerminal(2) = true
I1104 17:23:02.717302   50934 root.go:338] Updating PATH: /Users/jonesn/.minikube/bin
I1104 17:23:02.717650   50934 out.go:352] Setting JSON to false
I1104 17:23:02.763393   50934 start.go:129] hostinfo: {"hostname":"NigelMacBook.cherrybyte.me.uk","uptime":359286,"bootTime":1730381696,"procs":876,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.2","kernelVersion":"24.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"e5a00287-9a39-555d-9fe7-c43f261b9511"}
W1104 17:23:02.763495   50934 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1104 17:23:02.772899   50934 out.go:177] üòÑ  minikube v1.34.0 on Darwin 15.2 (arm64)
I1104 17:23:02.782354   50934 notify.go:220] Checking for updates...
I1104 17:23:02.782748   50934 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=crio, KubernetesVersion=v1.31.0
I1104 17:23:02.782820   50934 driver.go:394] Setting default libvirt URI to qemu:///system
I1104 17:23:02.913085   50934 podman.go:123] podman version: 5.2.5
I1104 17:23:02.917507   50934 out.go:177] ‚ú®  Using the podman (experimental) driver based on existing profile
I1104 17:23:02.928351   50934 start.go:297] selected driver: podman
I1104 17:23:02.928357   50934 start.go:901] validating driver "podman" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7864 CPUs:2 DiskSize:20000 Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:crio CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:crio ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 17:23:02.928450   50934 start.go:912] status for podman: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1104 17:23:02.928551   50934 cli_runner.go:164] Run: podman system info --format json
I1104 17:23:03.053567   50934 info.go:288] podman info: {Host:{BuildahVersion:1.37.5 CgroupVersion:v2 Conmon:{Package:conmon-2.1.12-2.fc40.aarch64 Path:/usr/bin/conmon Version:conmon version 2.1.12, commit: } Distribution:{Distribution:fedora Version:40} MemFree:1155694592 MemTotal:8296878080 OCIRuntime:{Name:crun Package:crun-1.17-1.fc40.aarch64 Path:/usr/bin/crun Version:crun version 1.17
commit: 000fa0d4eeed8938301f3bcf8206405315bc1017
rundir: /run/crun
spec: 1.0.0
+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL} SwapFree:0 SwapTotal:0 Arch:arm64 Cpus:5 Eventlogger:journald Hostname:localhost.localdomain Kernel:6.11.3-200.fc40.aarch64 Os:linux Security:{Rootless:false} Uptime:4h 11m 15.00s (Approximately 0.17 days)} Registries:{Search:[docker.io]} Store:{ConfigFile:/usr/share/containers/storage.conf ContainerStore:{Number:21} GraphDriverName:overlay GraphOptions:{} GraphRoot:/var/lib/containers/storage GraphStatus:{BackingFilesystem:xfs NativeOverlayDiff:false SupportsDType:true UsingMetacopy:true} ImageStore:{Number:58} RunRoot:/run/containers/storage VolumePath:/var/lib/containers/storage/volumes}}
I1104 17:23:03.063565   50934 cni.go:84] Creating CNI manager for ""
I1104 17:23:03.063578   50934 cni.go:143] "podman" driver + "crio" runtime found, recommending kindnet
I1104 17:23:03.063619   50934 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7864 CPUs:2 DiskSize:20000 Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:crio CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:crio ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 17:23:03.072459   50934 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1104 17:23:03.076346   50934 cache.go:121] Beginning downloading kic base image for podman with crio
I1104 17:23:03.080575   50934 out.go:177] üöú  Pulling base image v0.0.45 ...
I1104 17:23:03.092526   50934 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime crio
I1104 17:23:03.092546   50934 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1104 17:23:03.092568   50934 preload.go:146] Found local preload: /Users/jonesn/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-cri-o-overlay-arm64.tar.lz4
I1104 17:23:03.092575   50934 cache.go:56] Caching tarball of preloaded images
I1104 17:23:03.092694   50934 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1104 17:23:03.092705   50934 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1104 17:23:03.092708   50934 preload.go:172] Found /Users/jonesn/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-cri-o-overlay-arm64.tar.lz4 in cache, skipping download
I1104 17:23:03.092709   50934 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1104 17:23:03.092725   50934 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1104 17:23:03.092726   50934 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on crio
I1104 17:23:03.092784   50934 profile.go:143] Saving config to /Users/jonesn/.minikube/profiles/minikube/config.json ...
E1104 17:23:03.093509   50934 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1104 17:23:03.093530   50934 cache.go:194] Successfully downloaded all kic artifacts
I1104 17:23:03.093549   50934 start.go:360] acquireMachinesLock for minikube: {Name:mk5b18f2c34f3ea203026816552933ded9ef7440 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1104 17:23:03.093604   50934 start.go:364] duration metric: took 45.166¬µs to acquireMachinesLock for "minikube"
I1104 17:23:03.093615   50934 start.go:96] Skipping create...Using existing machine configuration
I1104 17:23:03.093617   50934 fix.go:54] fixHost starting: 
I1104 17:23:03.093852   50934 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I1104 17:23:03.189864   50934 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1104 17:23:03.189911   50934 fix.go:138] unexpected machine state, will restart: <nil>
I1104 17:23:03.195488   50934 out.go:177] üèÉ  Updating the running podman "minikube" container ...
I1104 17:23:03.203342   50934 machine.go:93] provisionDockerMachine start ...
I1104 17:23:03.203447   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:03.321773   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:03.421045   50934 main.go:141] libmachine: Using SSH client type: native
I1104 17:23:03.421576   50934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102acc5a0] 0x102acee00 <nil>  [] 0s} 127.0.0.1 36433 <nil> <nil>}
I1104 17:23:03.421582   50934 main.go:141] libmachine: About to run SSH command:
hostname
I1104 17:23:03.483349   50934 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1104 17:23:03.484351   50934 ubuntu.go:169] provisioning hostname "minikube"
I1104 17:23:03.484439   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:03.614465   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:03.729096   50934 main.go:141] libmachine: Using SSH client type: native
I1104 17:23:03.729337   50934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102acc5a0] 0x102acee00 <nil>  [] 0s} 127.0.0.1 36433 <nil> <nil>}
I1104 17:23:03.729351   50934 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1104 17:23:03.800779   50934 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1104 17:23:03.800877   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:03.933129   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:04.020232   50934 main.go:141] libmachine: Using SSH client type: native
I1104 17:23:04.020435   50934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102acc5a0] 0x102acee00 <nil>  [] 0s} 127.0.0.1 36433 <nil> <nil>}
I1104 17:23:04.020441   50934 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1104 17:23:04.086759   50934 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1104 17:23:04.086772   50934 ubuntu.go:175] set auth options {CertDir:/Users/jonesn/.minikube CaCertPath:/Users/jonesn/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/jonesn/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/jonesn/.minikube/machines/server.pem ServerKeyPath:/Users/jonesn/.minikube/machines/server-key.pem ClientKeyPath:/Users/jonesn/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/jonesn/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/jonesn/.minikube}
I1104 17:23:04.086793   50934 ubuntu.go:177] setting up certificates
I1104 17:23:04.086798   50934 provision.go:84] configureAuth start
I1104 17:23:04.086853   50934 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I1104 17:23:04.182504   50934 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1104 17:23:04.280573   50934 provision.go:143] copyHostCerts
I1104 17:23:04.280693   50934 exec_runner.go:144] found /Users/jonesn/.minikube/ca.pem, removing ...
I1104 17:23:04.280710   50934 exec_runner.go:203] rm: /Users/jonesn/.minikube/ca.pem
I1104 17:23:04.281388   50934 exec_runner.go:151] cp: /Users/jonesn/.minikube/certs/ca.pem --> /Users/jonesn/.minikube/ca.pem (1078 bytes)
I1104 17:23:04.281877   50934 exec_runner.go:144] found /Users/jonesn/.minikube/cert.pem, removing ...
I1104 17:23:04.281881   50934 exec_runner.go:203] rm: /Users/jonesn/.minikube/cert.pem
I1104 17:23:04.282193   50934 exec_runner.go:151] cp: /Users/jonesn/.minikube/certs/cert.pem --> /Users/jonesn/.minikube/cert.pem (1123 bytes)
I1104 17:23:04.282577   50934 exec_runner.go:144] found /Users/jonesn/.minikube/key.pem, removing ...
I1104 17:23:04.282579   50934 exec_runner.go:203] rm: /Users/jonesn/.minikube/key.pem
I1104 17:23:04.282862   50934 exec_runner.go:151] cp: /Users/jonesn/.minikube/certs/key.pem --> /Users/jonesn/.minikube/key.pem (1675 bytes)
I1104 17:23:04.283240   50934 provision.go:117] generating server cert: /Users/jonesn/.minikube/machines/server.pem ca-key=/Users/jonesn/.minikube/certs/ca.pem private-key=/Users/jonesn/.minikube/certs/ca-key.pem org=jonesn.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1104 17:23:04.438273   50934 provision.go:177] copyRemoteCerts
I1104 17:23:04.438579   50934 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1104 17:23:04.438662   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:04.555751   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:04.640087   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:04.677234   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1104 17:23:04.692785   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1104 17:23:04.707944   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1104 17:23:04.723369   50934 provision.go:87] duration metric: took 636.3215ms to configureAuth
I1104 17:23:04.723381   50934 ubuntu.go:193] setting minikube options for container-runtime
I1104 17:23:04.723551   50934 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=crio, KubernetesVersion=v1.31.0
I1104 17:23:04.723647   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:04.847942   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:04.931616   50934 main.go:141] libmachine: Using SSH client type: native
I1104 17:23:04.931833   50934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102acc5a0] 0x102acee00 <nil>  [] 0s} 127.0.0.1 36433 <nil> <nil>}
I1104 17:23:04.931843   50934 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /etc/sysconfig && printf %s "
CRIO_MINIKUBE_OPTIONS='--insecure-registry 10.96.0.0/12 '
" | sudo tee /etc/sysconfig/crio.minikube && sudo systemctl restart crio
I1104 17:23:05.152339   50934 main.go:141] libmachine: SSH cmd err, output: <nil>: 
CRIO_MINIKUBE_OPTIONS='--insecure-registry 10.96.0.0/12 '

I1104 17:23:05.152353   50934 machine.go:96] duration metric: took 1.948994542s to provisionDockerMachine
I1104 17:23:05.152359   50934 start.go:293] postStartSetup for "minikube" (driver="podman")
I1104 17:23:05.152365   50934 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1104 17:23:05.152444   50934 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1104 17:23:05.152473   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:05.279390   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:05.358159   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:05.393717   50934 ssh_runner.go:195] Run: cat /etc/os-release
I1104 17:23:05.396098   50934 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1104 17:23:05.396115   50934 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1104 17:23:05.396118   50934 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1104 17:23:05.396122   50934 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1104 17:23:05.396127   50934 filesync.go:126] Scanning /Users/jonesn/.minikube/addons for local assets ...
I1104 17:23:05.396250   50934 filesync.go:126] Scanning /Users/jonesn/.minikube/files for local assets ...
I1104 17:23:05.396290   50934 start.go:296] duration metric: took 243.926458ms for postStartSetup
I1104 17:23:05.396709   50934 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1104 17:23:05.396741   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:05.517210   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:05.601992   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:05.636662   50934 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1104 17:23:05.639988   50934 fix.go:56] duration metric: took 2.546356s for fixHost
I1104 17:23:05.640004   50934 start.go:83] releasing machines lock for "minikube", held for 2.546381667s
I1104 17:23:05.640131   50934 cli_runner.go:164] Run: podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I1104 17:23:05.727584   50934 cli_runner.go:164] Run: podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1104 17:23:05.816939   50934 ssh_runner.go:195] Run: cat /version.json
I1104 17:23:05.817012   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:05.817036   50934 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1104 17:23:05.817090   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:05.940891   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:05.943322   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:06.033527   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:06.035051   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:06.145283   50934 ssh_runner.go:195] Run: systemctl --version
I1104 17:23:06.148437   50934 ssh_runner.go:195] Run: sudo sh -c "podman version >/dev/null"
I1104 17:23:06.272433   50934 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1104 17:23:06.276321   50934 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1104 17:23:06.282904   50934 cni.go:221] loopback cni configuration disabled: "/etc/cni/net.d/*loopback.conf*" found
I1104 17:23:06.282990   50934 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1104 17:23:06.289305   50934 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1104 17:23:06.289316   50934 start.go:495] detecting cgroup driver to use...
I1104 17:23:06.289331   50934 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1104 17:23:06.289653   50934 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1104 17:23:06.298010   50934 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1104 17:23:06.305579   50934 docker.go:217] disabling cri-docker service (if available) ...
I1104 17:23:06.305651   50934 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.socket
I1104 17:23:06.313504   50934 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.service
I1104 17:23:06.320805   50934 ssh_runner.go:195] Run: sudo systemctl disable cri-docker.socket
I1104 17:23:06.381502   50934 ssh_runner.go:195] Run: sudo systemctl mask cri-docker.service
I1104 17:23:06.442087   50934 docker.go:233] disabling docker service ...
I1104 17:23:06.442165   50934 ssh_runner.go:195] Run: sudo systemctl stop -f docker.socket
I1104 17:23:06.450571   50934 ssh_runner.go:195] Run: sudo systemctl stop -f docker.service
I1104 17:23:06.458550   50934 ssh_runner.go:195] Run: sudo systemctl disable docker.socket
I1104 17:23:06.522037   50934 ssh_runner.go:195] Run: sudo systemctl mask docker.service
I1104 17:23:06.589123   50934 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1104 17:23:06.597646   50934 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/crio/crio.sock
" | sudo tee /etc/crictl.yaml"
I1104 17:23:06.608076   50934 crio.go:59] configure cri-o to use "registry.k8s.io/pause:3.10" pause image...
I1104 17:23:06.608168   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|^.*pause_image = .*$|pause_image = "registry.k8s.io/pause:3.10"|' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.615958   50934 crio.go:70] configuring cri-o to use "cgroupfs" as cgroup driver...
I1104 17:23:06.616064   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|^.*cgroup_manager = .*$|cgroup_manager = "cgroupfs"|' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.623622   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i '/conmon_cgroup = .*/d' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.630378   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i '/cgroup_manager = .*/a conmon_cgroup = "pod"' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.638080   50934 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1104 17:23:06.644692   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *"net.ipv4.ip_unprivileged_port_start=.*"/d' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.651616   50934 ssh_runner.go:195] Run: sh -c "sudo grep -q "^ *default_sysctls" /etc/crio/crio.conf.d/02-crio.conf || sudo sed -i '/conmon_cgroup = .*/a default_sysctls = \[\n\]' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.658844   50934 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^default_sysctls *= *\[|&\n  "net.ipv4.ip_unprivileged_port_start=0",|' /etc/crio/crio.conf.d/02-crio.conf"
I1104 17:23:06.665905   50934 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1104 17:23:06.671466   50934 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1104 17:23:06.677564   50934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 17:23:06.735696   50934 ssh_runner.go:195] Run: sudo systemctl restart crio
I1104 17:23:06.868970   50934 start.go:542] Will wait 60s for socket path /var/run/crio/crio.sock
I1104 17:23:06.869327   50934 ssh_runner.go:195] Run: stat /var/run/crio/crio.sock
I1104 17:23:06.872760   50934 start.go:563] Will wait 60s for crictl version
I1104 17:23:06.873054   50934 ssh_runner.go:195] Run: which crictl
I1104 17:23:06.875044   50934 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1104 17:23:06.900379   50934 start.go:579] Version:  0.1.0
RuntimeName:  cri-o
RuntimeVersion:  1.24.6
RuntimeApiVersion:  v1
I1104 17:23:06.900761   50934 ssh_runner.go:195] Run: crio --version
I1104 17:23:06.927306   50934 ssh_runner.go:195] Run: crio --version
I1104 17:23:06.971340   50934 out.go:177] üéÅ  Preparing Kubernetes v1.31.0 on CRI-O 1.24.6 ...
E1104 17:23:06.975357   50934 start.go:132] Unable to get host IP: RoutableHostIPFromInside is currently only implemented for linux
I1104 17:23:06.975473   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:07.106539   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 17:23:07.192680   50934 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7864 CPUs:2 DiskSize:20000 Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:crio CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:crio ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1104 17:23:07.192763   50934 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime crio
I1104 17:23:07.192850   50934 ssh_runner.go:195] Run: sudo crictl images --output json
I1104 17:23:07.216632   50934 crio.go:514] all images are preloaded for cri-o runtime.
I1104 17:23:07.216643   50934 crio.go:433] Images already preloaded, skipping extraction
I1104 17:23:07.216735   50934 ssh_runner.go:195] Run: sudo crictl images --output json
I1104 17:23:07.236152   50934 crio.go:514] all images are preloaded for cri-o runtime.
I1104 17:23:07.236159   50934 cache_images.go:84] Images are preloaded, skipping loading
I1104 17:23:07.236163   50934 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 crio true true} ...
I1104 17:23:07.236260   50934 kubeadm.go:946] kubelet [Unit]
Wants=crio.service

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:crio CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1104 17:23:07.236394   50934 ssh_runner.go:195] Run: crio config
I1104 17:23:07.261239   50934 cni.go:84] Creating CNI manager for ""
I1104 17:23:07.261244   50934 cni.go:143] "podman" driver + "crio" runtime found, recommending kindnet
I1104 17:23:07.261249   50934 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1104 17:23:07.261262   50934 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/crio/crio.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/crio/crio.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1104 17:23:07.261348   50934 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/crio/crio.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/crio/crio.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1104 17:23:07.261468   50934 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1104 17:23:07.267094   50934 binaries.go:44] Found k8s binaries, skipping transfer
I1104 17:23:07.267178   50934 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1104 17:23:07.273171   50934 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (306 bytes)
I1104 17:23:07.284686   50934 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1104 17:23:07.296251   50934 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2146 bytes)
I1104 17:23:07.307107   50934 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1104 17:23:07.309334   50934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 17:23:07.365629   50934 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1104 17:23:07.373325   50934 certs.go:68] Setting up /Users/jonesn/.minikube/profiles/minikube for IP: 192.168.49.2
I1104 17:23:07.373329   50934 certs.go:194] generating shared ca certs ...
I1104 17:23:07.373339   50934 certs.go:226] acquiring lock for ca certs: {Name:mkad62b6d8e6c069c5f416a110786a49d7559da1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 17:23:07.373551   50934 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/jonesn/.minikube/ca.key
I1104 17:23:07.373832   50934 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/jonesn/.minikube/proxy-client-ca.key
I1104 17:23:07.373844   50934 certs.go:256] generating profile certs ...
I1104 17:23:07.373941   50934 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/jonesn/.minikube/profiles/minikube/client.key
I1104 17:23:07.374211   50934 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/jonesn/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1104 17:23:07.374417   50934 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/jonesn/.minikube/profiles/minikube/proxy-client.key
I1104 17:23:07.374622   50934 certs.go:484] found cert: /Users/jonesn/.minikube/certs/ca-key.pem (1675 bytes)
I1104 17:23:07.374662   50934 certs.go:484] found cert: /Users/jonesn/.minikube/certs/ca.pem (1078 bytes)
I1104 17:23:07.374702   50934 certs.go:484] found cert: /Users/jonesn/.minikube/certs/cert.pem (1123 bytes)
I1104 17:23:07.374732   50934 certs.go:484] found cert: /Users/jonesn/.minikube/certs/key.pem (1675 bytes)
I1104 17:23:07.376105   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1104 17:23:07.391823   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1104 17:23:07.407147   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1104 17:23:07.422193   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1104 17:23:07.438067   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1104 17:23:07.453221   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1104 17:23:07.468264   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1104 17:23:07.482889   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1104 17:23:07.498061   50934 ssh_runner.go:362] scp /Users/jonesn/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1104 17:23:07.512885   50934 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1104 17:23:07.524929   50934 ssh_runner.go:195] Run: openssl version
I1104 17:23:07.528105   50934 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1104 17:23:07.534144   50934 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1104 17:23:07.536745   50934 certs.go:528] hashing: -rw-r--r--. 1 root root 1111 Nov  4 14:57 /usr/share/ca-certificates/minikubeCA.pem
I1104 17:23:07.536796   50934 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1104 17:23:07.541057   50934 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1104 17:23:07.546903   50934 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1104 17:23:07.549576   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1104 17:23:07.553431   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1104 17:23:07.557134   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1104 17:23:07.560716   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1104 17:23:07.564347   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1104 17:23:07.568307   50934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1104 17:23:07.572385   50934 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7864 CPUs:2 DiskSize:20000 Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:crio CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:crio ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 17:23:07.572429   50934 cri.go:54] listing CRI containers in root : {State:paused Name: Namespaces:[kube-system]}
I1104 17:23:07.572788   50934 ssh_runner.go:195] Run: sudo -s eval "crictl ps -a --quiet --label io.kubernetes.pod.namespace=kube-system"
I1104 17:23:07.593232   50934 cri.go:89] found id: "ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07"
I1104 17:23:07.593240   50934 cri.go:89] found id: "b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621"
I1104 17:23:07.593242   50934 cri.go:89] found id: "e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab"
I1104 17:23:07.593243   50934 cri.go:89] found id: "47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6"
I1104 17:23:07.593252   50934 cri.go:89] found id: "229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454"
I1104 17:23:07.593254   50934 cri.go:89] found id: "b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979"
I1104 17:23:07.593254   50934 cri.go:89] found id: ""
I1104 17:23:07.593350   50934 ssh_runner.go:195] Run: sudo runc list -f json
I1104 17:23:07.606018   50934 cri.go:116] JSON = [{"ociVersion":"1.0.2-dev","id":"229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454","pid":1262,"status":"running","bundle":"/run/containers/storage/overlay-containers/229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454/userdata","rootfs":"/var/lib/containers/storage/overlay/c93d16be3f15b8f8d7584738c8651a0cc45f1256ab4d31f7351fad33fab80e46/merged","created":"2024-11-04T17:22:13.272910377Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"f8fb4364","io.kubernetes.container.name":"kube-scheduler","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"f8fb4364\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:22:13.249482859Z","io.kubernetes.cri-o.Image":"fbbbd428abb4dae52ab3018797d00d5840a739f0cc5697b662791831a60b0adb","io.kubernetes.cri-o.ImageName":"registry.k8s.io/kube-scheduler:v1.31.0","io.kubernetes.cri-o.ImageRef":"fbbbd428abb4dae52ab3018797d00d5840a739f0cc5697b662791831a60b0adb","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"kube-scheduler\",\"io.kubernetes.pod.name\":\"kube-scheduler-minikube\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"e039200acb850c82bb901653cc38ff6e\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_kube-scheduler-minikube_e039200acb850c82bb901653cc38ff6e/kube-scheduler/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"kube-scheduler\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/c93d16be3f15b8f8d7584738c8651a0cc45f1256ab4d31f7351fad33fab80e46/merged","io.kubernetes.cri-o.Name":"k8s_kube-scheduler_kube-scheduler-minikube_kube-system_e039200acb850c82bb901653cc38ff6e_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/c9f6b752a65d0e18dcce88f0d87a2ae1d5b251791ac5660e49df3c1d4a47d6ef/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"c9f6b752a65d0e18dcce88f0d87a2ae1d5b251791ac5660e49df3c1d4a47d6ef","io.kubernetes.cri-o.SandboxName":"k8s_kube-scheduler-minikube_kube-system_e039200acb850c82bb901653cc38ff6e_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/e039200acb850c82bb901653cc38ff6e/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/e039200acb850c82bb901653cc38ff6e/containers/kube-scheduler/31b1da61\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/kubernetes/scheduler.conf\",\"host_path\":\"/etc/kubernetes/scheduler.conf\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"kube-scheduler-minikube","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"e039200acb850c82bb901653cc38ff6e","kubernetes.io/config.hash":"e039200acb850c82bb901653cc38ff6e","kubernetes.io/config.seen":"2024-11-04T17:22:12.696437813Z","kubernetes.io/config.source":"file"},"owner":"root"},{"ociVersion":"1.0.2-dev","id":"47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6","pid":1280,"status":"running","bundle":"/run/containers/storage/overlay-containers/47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6/userdata","rootfs":"/var/lib/containers/storage/overlay/4b7f69b4e5facbba4c538481c1ca21d99bd40ae0cab518ed2c79f631fed60b55/merged","created":"2024-11-04T17:22:13.277043236Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"cdf7d3fa","io.kubernetes.container.name":"etcd","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"cdf7d3fa\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:22:13.260100632Z","io.kubernetes.cri-o.Image":"27e3830e1402783674d8b594038967deea9d51f0d91b34c93c8f39d2f68af7da","io.kubernetes.cri-o.ImageName":"registry.k8s.io/etcd:3.5.15-0","io.kubernetes.cri-o.ImageRef":"27e3830e1402783674d8b594038967deea9d51f0d91b34c93c8f39d2f68af7da","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"etcd\",\"io.kubernetes.pod.name\":\"etcd-minikube\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"a5363f4f31e043bdae3c93aca4991903\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_etcd-minikube_a5363f4f31e043bdae3c93aca4991903/etcd/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"etcd\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/4b7f69b4e5facbba4c538481c1ca21d99bd40ae0cab518ed2c79f631fed60b55/merged","io.kubernetes.cri-o.Name":"k8s_etcd_etcd-minikube_kube-system_a5363f4f31e043bdae3c93aca4991903_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/2e24244b7a081fa6546f1f7ed071cf06c0b6b40d10c9588d66d3391cdc23a85e/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"2e24244b7a081fa6546f1f7ed071cf06c0b6b40d10c9588d66d3391cdc23a85e","io.kubernetes.cri-o.SandboxName":"k8s_etcd-minikube_kube-system_a5363f4f31e043bdae3c93aca4991903_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/a5363f4f31e043bdae3c93aca4991903/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/a5363f4f31e043bdae3c93aca4991903/containers/etcd/0fd2f257\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/lib/minikube/etcd\",\"host_path\":\"/var/lib/minikube/etcd\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/lib/minikube/certs/etcd\",\"host_path\":\"/var/lib/minikube/certs/etcd\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"etcd-minikube","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"a5363f4f31e043bdae3c93aca4991903","kubeadm.kubernetes.io/etcd.advertise-client-urls":"https://192.168.49.2:2379","kubernetes.io/config.hash":"a5363f4f31e043bdae3c93aca4991903","kubernetes.io/config.seen":"2024-11-04T17:22:12.696432813Z","kubernetes.io/config.source":"file"},"owner":"root"},{"ociVersion":"1.0.2-dev","id":"b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979","pid":1251,"status":"running","bundle":"/run/containers/storage/overlay-containers/b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979/userdata","rootfs":"/var/lib/containers/storage/overlay/d161649c1fd07ece00ecdd4d73c6883d2ee87d1c5c43e91e5f36e0a96005ed2d/merged","created":"2024-11-04T17:22:13.27331188Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"3994b1a4","io.kubernetes.container.name":"kube-controller-manager","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"3994b1a4\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:22:13.248983481Z","io.kubernetes.cri-o.Image":"fcb0683e6bdbd083710cf2d6fd7eb699c77fe4994c38a5c82d059e2e3cb4c2fd","io.kubernetes.cri-o.ImageName":"registry.k8s.io/kube-controller-manager:v1.31.0","io.kubernetes.cri-o.ImageRef":"fcb0683e6bdbd083710cf2d6fd7eb699c77fe4994c38a5c82d059e2e3cb4c2fd","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"kube-controller-manager\",\"io.kubernetes.pod.name\":\"kube-controller-manager-minikube\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"40f5f661ab65f2e4bfe41ac2993c01de\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_kube-controller-manager-minikube_40f5f661ab65f2e4bfe41ac2993c01de/kube-controller-manager/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"kube-controller-manager\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/d161649c1fd07ece00ecdd4d73c6883d2ee87d1c5c43e91e5f36e0a96005ed2d/merged","io.kubernetes.cri-o.Name":"k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_40f5f661ab65f2e4bfe41ac2993c01de_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/ba4753157937c866fadd33db1eab0d155d7d8dde8fe36800a710fbfc6faa6863/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"ba4753157937c866fadd33db1eab0d155d7d8dde8fe36800a710fbfc6faa6863","io.kubernetes.cri-o.SandboxName":"k8s_kube-controller-manager-minikube_kube-system_40f5f661ab65f2e4bfe41ac2993c01de_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/etc/ca-certificates\",\"host_path\":\"/etc/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/40f5f661ab65f2e4bfe41ac2993c01de/containers/kube-controller-manager/5cec2433\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/40f5f661ab65f2e4bfe41ac2993c01de/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/ssl/certs\",\"host_path\":\"/etc/ssl/certs\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/kubernetes/controller-manager.conf\",\"host_path\":\"/etc/kubernetes/controller-manager.conf\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/usr/share/ca-certificates\",\"host_path\":\"/usr/share/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/lib/minikube/certs\",\"host_path\":\"/var/lib/minikube/certs\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/usr/local/share/ca-certificates\",\"host_path\":\"/usr/local/share/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/usr/libexec/kubernetes/kubelet-plugins/volume/exec\",\"host_path\":\"/usr/libexec/kubernetes/kubelet-plugins/volume/exec\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"kube-controller-manager-minikube","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"40f5f661ab65f2e4bfe41ac2993c01de","kubernetes.io/config.hash":"40f5f661ab65f2e4bfe41ac2993c01de","kubernetes.io/config.seen":"2024-11-04T17:22:12.696437354Z","kubernetes.io/config.source":"file"},"owner":"root"},{"ociVersion":"1.0.2-dev","id":"b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621","pid":1550,"status":"running","bundle":"/run/containers/storage/overlay-containers/b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621/userdata","rootfs":"/var/lib/containers/storage/overlay/427957554b0d7384990b8b7586541c34b4e641c2849d95af0931b8014a2d09d9/merged","created":"2024-11-04T17:22:22.929261515Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"78ccb3c","io.kubernetes.container.name":"kube-proxy","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"78ccb3c\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:22:22.902807061Z","io.kubernetes.cri-o.Image":"71d55d66fd4eec8986225089a135fadd96bc6624d987096808772ce1e1924d89","io.kubernetes.cri-o.ImageName":"registry.k8s.io/kube-proxy:v1.31.0","io.kubernetes.cri-o.ImageRef":"71d55d66fd4eec8986225089a135fadd96bc6624d987096808772ce1e1924d89","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"kube-proxy\",\"io.kubernetes.pod.name\":\"kube-proxy-xj8wm\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"ba76aa81-513b-4265-add8-ca3b67e2924e\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_kube-proxy-xj8wm_ba76aa81-513b-4265-add8-ca3b67e2924e/kube-proxy/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"kube-proxy\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/427957554b0d7384990b8b7586541c34b4e641c2849d95af0931b8014a2d09d9/merged","io.kubernetes.cri-o.Name":"k8s_kube-proxy_kube-proxy-xj8wm_kube-system_ba76aa81-513b-4265-add8-ca3b67e2924e_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/4c6f7a456b5ea99d5d2cffcc9cdc4410463165e92706fe967b1f93fa59ad0d86/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"4c6f7a456b5ea99d5d2cffcc9cdc4410463165e92706fe967b1f93fa59ad0d86","io.kubernetes.cri-o.SandboxName":"k8s_kube-proxy-xj8wm_kube-system_ba76aa81-513b-4265-add8-ca3b67e2924e_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/run/xtables.lock\",\"host_path\":\"/run/xtables.lock\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/lib/modules\",\"host_path\":\"/lib/modules\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/ba76aa81-513b-4265-add8-ca3b67e2924e/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/ba76aa81-513b-4265-add8-ca3b67e2924e/containers/kube-proxy/3a4a9ded\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/lib/kube-proxy\",\"host_path\":\"/var/lib/kubelet/pods/ba76aa81-513b-4265-add8-ca3b67e2924e/volumes/kubernetes.io~configmap/kube-proxy\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"host_path\":\"/var/lib/kubelet/pods/ba76aa81-513b-4265-add8-ca3b67e2924e/volumes/kubernetes.io~projected/kube-api-access-nprsn\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"kube-proxy-xj8wm","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"ba76aa81-513b-4265-add8-ca3b67e2924e","kubernetes.io/config.seen":"2024-11-04T17:22:21.967870973Z","kubernetes.io/config.source":"api"},"owner":"root"},{"ociVersion":"1.0.2-dev","id":"e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab","pid":1292,"status":"running","bundle":"/run/containers/storage/overlay-containers/e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab/userdata","rootfs":"/var/lib/containers/storage/overlay/d2dbe48907b6eb08a8eb896249f9d9c25e020a6bb5f086fee2215a93cb723647/merged","created":"2024-11-04T17:22:13.283525317Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"f72d0944","io.kubernetes.container.name":"kube-apiserver","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"f72d0944\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:22:13.260898429Z","io.kubernetes.cri-o.Image":"cd0f0ae0ec9e0cdc092079156c122bf034ba3f24d31c1b1dd1b52a42ecf9b388","io.kubernetes.cri-o.ImageName":"registry.k8s.io/kube-apiserver:v1.31.0","io.kubernetes.cri-o.ImageRef":"cd0f0ae0ec9e0cdc092079156c122bf034ba3f24d31c1b1dd1b52a42ecf9b388","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"kube-apiserver\",\"io.kubernetes.pod.name\":\"kube-apiserver-minikube\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"9e315b3a91fa9f6f7463439d9dac1a56\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_kube-apiserver-minikube_9e315b3a91fa9f6f7463439d9dac1a56/kube-apiserver/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"kube-apiserver\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/d2dbe48907b6eb08a8eb896249f9d9c25e020a6bb5f086fee2215a93cb723647/merged","io.kubernetes.cri-o.Name":"k8s_kube-apiserver_kube-apiserver-minikube_kube-system_9e315b3a91fa9f6f7463439d9dac1a56_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/6be0c64796923d18a4d52e8a706500667a44183366615b5fe9259835d2e300e4/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"6be0c64796923d18a4d52e8a706500667a44183366615b5fe9259835d2e300e4","io.kubernetes.cri-o.SandboxName":"k8s_kube-apiserver-minikube_kube-system_9e315b3a91fa9f6f7463439d9dac1a56_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/9e315b3a91fa9f6f7463439d9dac1a56/containers/kube-apiserver/58aa5a8b\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/ca-certificates\",\"host_path\":\"/etc/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/9e315b3a91fa9f6f7463439d9dac1a56/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/usr/share/ca-certificates\",\"host_path\":\"/usr/share/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/ssl/certs\",\"host_path\":\"/etc/ssl/certs\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/lib/minikube/certs\",\"host_path\":\"/var/lib/minikube/certs\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/usr/local/share/ca-certificates\",\"host_path\":\"/usr/local/share/ca-certificates\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"kube-apiserver-minikube","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"9e315b3a91fa9f6f7463439d9dac1a56","kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint":"192.168.49.2:8443","kubernetes.io/config.hash":"9e315b3a91fa9f6f7463439d9dac1a56","kubernetes.io/config.seen":"2024-11-04T17:22:12.696436354Z","kubernetes.io/config.source":"file"},"owner":"root"},{"ociVersion":"1.0.2-dev","id":"ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07","pid":1878,"status":"running","bundle":"/run/containers/storage/overlay-containers/ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07/userdata","rootfs":"/var/lib/containers/storage/overlay/e173c0792c35d1e862681d988dfe5469815ed576092b1325afcc2f652be8ef73/merged","created":"2024-11-04T17:23:05.624667853Z","annotations":{"io.container.manager":"cri-o","io.kubernetes.container.hash":"6c6bf961","io.kubernetes.container.name":"storage-provisioner","io.kubernetes.container.restartCount":"0","io.kubernetes.container.terminationMessagePath":"/dev/termination-log","io.kubernetes.container.terminationMessagePolicy":"File","io.kubernetes.cri-o.Annotations":"{\"io.kubernetes.container.hash\":\"6c6bf961\",\"io.kubernetes.container.restartCount\":\"0\",\"io.kubernetes.container.terminationMessagePath\":\"/dev/termination-log\",\"io.kubernetes.container.terminationMessagePolicy\":\"File\",\"io.kubernetes.pod.terminationGracePeriod\":\"30\"}","io.kubernetes.cri-o.ContainerID":"ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07","io.kubernetes.cri-o.ContainerType":"container","io.kubernetes.cri-o.Created":"2024-11-04T17:23:05.60007662Z","io.kubernetes.cri-o.Image":"ba04bb24b95753201135cbc420b233c1b0b9fa2e1fd21d28319c348c33fbcde6","io.kubernetes.cri-o.ImageName":"gcr.io/k8s-minikube/storage-provisioner:v5","io.kubernetes.cri-o.ImageRef":"ba04bb24b95753201135cbc420b233c1b0b9fa2e1fd21d28319c348c33fbcde6","io.kubernetes.cri-o.Labels":"{\"io.kubernetes.container.name\":\"storage-provisioner\",\"io.kubernetes.pod.name\":\"storage-provisioner\",\"io.kubernetes.pod.namespace\":\"kube-system\",\"io.kubernetes.pod.uid\":\"a47a1397-2b29-46ba-9d22-c33ea5ad0238\"}","io.kubernetes.cri-o.LogPath":"/var/log/pods/kube-system_storage-provisioner_a47a1397-2b29-46ba-9d22-c33ea5ad0238/storage-provisioner/0.log","io.kubernetes.cri-o.Metadata":"{\"name\":\"storage-provisioner\"}","io.kubernetes.cri-o.MountPoint":"/var/lib/containers/storage/overlay/e173c0792c35d1e862681d988dfe5469815ed576092b1325afcc2f652be8ef73/merged","io.kubernetes.cri-o.Name":"k8s_storage-provisioner_storage-provisioner_kube-system_a47a1397-2b29-46ba-9d22-c33ea5ad0238_0","io.kubernetes.cri-o.ResolvPath":"/run/containers/storage/overlay-containers/6d50d01d694edb7a855a523efb4df7a2f7387bb12e6287424acaac2bf1d65ba1/userdata/resolv.conf","io.kubernetes.cri-o.SandboxID":"6d50d01d694edb7a855a523efb4df7a2f7387bb12e6287424acaac2bf1d65ba1","io.kubernetes.cri-o.SandboxName":"k8s_storage-provisioner_kube-system_a47a1397-2b29-46ba-9d22-c33ea5ad0238_0","io.kubernetes.cri-o.SeccompProfilePath":"","io.kubernetes.cri-o.Stdin":"false","io.kubernetes.cri-o.StdinOnce":"false","io.kubernetes.cri-o.TTY":"false","io.kubernetes.cri-o.Volumes":"[{\"container_path\":\"/tmp\",\"host_path\":\"/tmp\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/etc/hosts\",\"host_path\":\"/var/lib/kubelet/pods/a47a1397-2b29-46ba-9d22-c33ea5ad0238/etc-hosts\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/dev/termination-log\",\"host_path\":\"/var/lib/kubelet/pods/a47a1397-2b29-46ba-9d22-c33ea5ad0238/containers/storage-provisioner/883fac65\",\"readonly\":false,\"propagation\":0,\"selinux_relabel\":false},{\"container_path\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"host_path\":\"/var/lib/kubelet/pods/a47a1397-2b29-46ba-9d22-c33ea5ad0238/volumes/kubernetes.io~projected/kube-api-access-nxhrf\",\"readonly\":true,\"propagation\":0,\"selinux_relabel\":false}]","io.kubernetes.pod.name":"storage-provisioner","io.kubernetes.pod.namespace":"kube-system","io.kubernetes.pod.terminationGracePeriod":"30","io.kubernetes.pod.uid":"a47a1397-2b29-46ba-9d22-c33ea5ad0238","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Reconcile\",\"integration-test\":\"storage-provisioner\"},\"name\":\"storage-provisioner\",\"namespace\":\"kube-system\"},\"spec\":{\"containers\":[{\"command\":[\"/storage-provisioner\"],\"image\":\"gcr.io/k8s-minikube/storage-provisioner:v5\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"storage-provisioner\",\"volumeMounts\":[{\"mountPath\":\"/tmp\",\"name\":\"tmp\"}]}],\"hostNetwork\":true,\"serviceAccountName\":\"storage-provisioner\",\"volumes\":[{\"hostPath\":{\"path\":\"/tmp\",\"type\":\"Directory\"},\"name\":\"tmp\"}]}}\n","kubernetes.io/config.seen":"2024-11-04T17:23:05.270278354Z","kubernetes.io/config.source":"api"},"owner":"root"}]
I1104 17:23:07.606227   50934 cri.go:126] list returned 6 containers
I1104 17:23:07.606232   50934 cri.go:129] container: {ID:229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454 Status:running}
I1104 17:23:07.606239   50934 cri.go:135] skipping {229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454 running}: state = "running", want "paused"
I1104 17:23:07.606243   50934 cri.go:129] container: {ID:47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6 Status:running}
I1104 17:23:07.606245   50934 cri.go:135] skipping {47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6 running}: state = "running", want "paused"
I1104 17:23:07.606247   50934 cri.go:129] container: {ID:b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979 Status:running}
I1104 17:23:07.606249   50934 cri.go:135] skipping {b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979 running}: state = "running", want "paused"
I1104 17:23:07.606251   50934 cri.go:129] container: {ID:b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621 Status:running}
I1104 17:23:07.606253   50934 cri.go:135] skipping {b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621 running}: state = "running", want "paused"
I1104 17:23:07.606254   50934 cri.go:129] container: {ID:e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab Status:running}
I1104 17:23:07.606256   50934 cri.go:135] skipping {e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab running}: state = "running", want "paused"
I1104 17:23:07.606258   50934 cri.go:129] container: {ID:ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07 Status:running}
I1104 17:23:07.606259   50934 cri.go:135] skipping {ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07 running}: state = "running", want "paused"
I1104 17:23:07.606359   50934 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1104 17:23:07.612376   50934 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1104 17:23:07.612387   50934 kubeadm.go:593] restartPrimaryControlPlane start ...
I1104 17:23:07.612475   50934 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1104 17:23:07.618481   50934 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1104 17:23:07.618546   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:07.742872   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 17:23:07.831191   50934 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:44205"
I1104 17:23:07.834339   50934 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1104 17:23:07.841207   50934 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1104 17:23:07.841219   50934 kubeadm.go:597] duration metric: took 228.828834ms to restartPrimaryControlPlane
I1104 17:23:07.841226   50934 kubeadm.go:394] duration metric: took 268.844708ms to StartCluster
I1104 17:23:07.841234   50934 settings.go:142] acquiring lock: {Name:mk1dc5aa7f8213ba240e49c577c50b73ecb56946 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 17:23:07.841340   50934 settings.go:150] Updating kubeconfig:  /Users/jonesn/.kube/config
I1104 17:23:07.844168   50934 lock.go:35] WriteFile acquiring /Users/jonesn/.kube/config: {Name:mke0d118e8c3c9423a14a32796bad1c0510f090a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 17:23:07.844716   50934 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:crio ControlPlane:true Worker:true}
I1104 17:23:07.844844   50934 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=crio, KubernetesVersion=v1.31.0
I1104 17:23:07.844735   50934 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1104 17:23:07.844968   50934 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1104 17:23:07.844966   50934 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1104 17:23:07.844981   50934 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1104 17:23:07.844983   50934 addons.go:243] addon storage-provisioner should already be in state true
I1104 17:23:07.844997   50934 host.go:66] Checking if "minikube" exists ...
I1104 17:23:07.845000   50934 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1104 17:23:07.845215   50934 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I1104 17:23:07.845347   50934 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I1104 17:23:07.849928   50934 out.go:177] üîé  Verifying Kubernetes components...
I1104 17:23:07.859887   50934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 17:23:07.935118   50934 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1104 17:23:07.944896   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:07.947275   50934 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1104 17:23:07.947447   50934 addons.go:243] addon default-storageclass should already be in state true
I1104 17:23:07.947463   50934 host.go:66] Checking if "minikube" exists ...
I1104 17:23:07.947732   50934 cli_runner.go:164] Run: podman container inspect minikube --format={{.State.Status}}
I1104 17:23:07.949887   50934 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1104 17:23:07.953818   50934 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1104 17:23:07.953823   50934 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1104 17:23:07.953884   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:08.041611   50934 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1104 17:23:08.041634   50934 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1104 17:23:08.041746   50934 cli_runner.go:164] Run: podman version --format {{.Version}}
I1104 17:23:08.081367   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:08.081456   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 17:23:08.170725   50934 cli_runner.go:164] Run: podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 17:23:08.172114   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:08.173556   50934 api_server.go:52] waiting for apiserver process to appear ...
I1104 17:23:08.173691   50934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1104 17:23:08.182733   50934 api_server.go:72] duration metric: took 337.996208ms to wait for apiserver process to appear ...
I1104 17:23:08.182741   50934 api_server.go:88] waiting for apiserver healthz status ...
I1104 17:23:08.182749   50934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:44205/healthz ...
I1104 17:23:08.186770   50934 api_server.go:279] https://127.0.0.1:44205/healthz returned 200:
ok
I1104 17:23:08.191573   50934 api_server.go:141] control plane version: v1.31.0
I1104 17:23:08.191583   50934 api_server.go:131] duration metric: took 8.839ms to wait for apiserver health ...
I1104 17:23:08.191588   50934 system_pods.go:43] waiting for kube-system pods to appear ...
I1104 17:23:08.196722   50934 system_pods.go:59] 8 kube-system pods found
I1104 17:23:08.196739   50934 system_pods.go:61] "coredns-6f6b679f8f-znkjh" [585ce8ec-311a-4a7b-a35f-b6896fdd10fd] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1104 17:23:08.196742   50934 system_pods.go:61] "etcd-minikube" [76f4b908-9d3a-42b6-b24b-f3f4c0414da1] Running
I1104 17:23:08.196744   50934 system_pods.go:61] "kindnet-gmrjn" [4a955413-0306-4f19-a554-1f7b20df5a0b] Pending / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I1104 17:23:08.196747   50934 system_pods.go:61] "kube-apiserver-minikube" [dfb7b2f1-fada-47c1-b817-0d6962e17fcc] Running
I1104 17:23:08.196749   50934 system_pods.go:61] "kube-controller-manager-minikube" [0bb7fcfa-be01-495b-9fec-32402c526f5b] Running
I1104 17:23:08.196750   50934 system_pods.go:61] "kube-proxy-xj8wm" [ba76aa81-513b-4265-add8-ca3b67e2924e] Running
I1104 17:23:08.196751   50934 system_pods.go:61] "kube-scheduler-minikube" [b71023b5-8361-421f-84f1-6acdba9be407] Running
I1104 17:23:08.196752   50934 system_pods.go:61] "storage-provisioner" [a47a1397-2b29-46ba-9d22-c33ea5ad0238] Running
I1104 17:23:08.196754   50934 system_pods.go:74] duration metric: took 5.163708ms to wait for pod list to return data ...
I1104 17:23:08.196758   50934 kubeadm.go:582] duration metric: took 352.02575ms to wait for: map[apiserver:true system_pods:true]
I1104 17:23:08.196764   50934 node_conditions.go:102] verifying NodePressure condition ...
I1104 17:23:08.198947   50934 node_conditions.go:122] node storage ephemeral capacity is 104266732Ki
I1104 17:23:08.198956   50934 node_conditions.go:123] node cpu capacity is 5
I1104 17:23:08.198961   50934 node_conditions.go:105] duration metric: took 2.195625ms to run NodePressure ...
I1104 17:23:08.198966   50934 start.go:241] waiting for startup goroutines ...
I1104 17:23:08.215708   50934 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1104 17:23:08.251894   50934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:36433 SSHKeyPath:/Users/jonesn/.minikube/machines/minikube/id_rsa Username:docker}
I1104 17:23:08.299456   50934 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1104 17:23:08.544545   50934 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1104 17:23:08.548535   50934 addons.go:510] duration metric: took 703.81025ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1104 17:23:08.548549   50934 start.go:246] waiting for cluster config update ...
I1104 17:23:08.548559   50934 start.go:255] writing updated cluster config ...
I1104 17:23:08.549247   50934 ssh_runner.go:195] Run: rm -f paused
I1104 17:23:08.682859   50934 start.go:600] kubectl: 1.30.1, cluster: 1.31.0 (minor skew: 1)
I1104 17:23:08.687597   50934 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> CRI-O <==
Nov 04 17:23:30 minikube crio[2074]: time="2024-11-04 17:23:30.804673484Z" level=info msg="Found CNI network kindnet (type=ptp) at /etc/cni/net.d/10-kindnet.conflist"
Nov 04 17:23:30 minikube crio[2074]: time="2024-11-04 17:23:30.804698735Z" level=info msg="Updated default CNI network name to kindnet"
Nov 04 17:23:30 minikube crio[2074]: time="2024-11-04 17:23:30.804707526Z" level=info msg="CNI monitoring event \"/etc/cni/net.d/10-kindnet.conflist\": CREATE"
Nov 04 17:23:30 minikube crio[2074]: time="2024-11-04 17:23:30.806937498Z" level=info msg="Found CNI network kindnet (type=ptp) at /etc/cni/net.d/10-kindnet.conflist"
Nov 04 17:23:30 minikube crio[2074]: time="2024-11-04 17:23:30.806956123Z" level=info msg="Updated default CNI network name to kindnet"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.589805338Z" level=info msg="Running pod sandbox: kube-system/coredns-6f6b679f8f-znkjh/POD" id=b84d5ae4-673f-4f86-aad6-29e502d074fa name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.589855922Z" level=warning msg="Allowed annotations are specified for workload []"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.604883889Z" level=info msg="Got pod network &{Name:coredns-6f6b679f8f-znkjh Namespace:kube-system ID:ddacf1250b4f6e2573cde51c6f10d0c8a856f564c7275fe6fe6e2caf68b3c1e4 UID:585ce8ec-311a-4a7b-a35f-b6896fdd10fd NetNS:/var/run/netns/724ead4f-d61c-461e-90d7-f416d4e52b44 Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.604916306Z" level=info msg="Adding pod kube-system_coredns-6f6b679f8f-znkjh to CNI network \"kindnet\" (type=ptp)"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.612127475Z" level=info msg="Got pod network &{Name:coredns-6f6b679f8f-znkjh Namespace:kube-system ID:ddacf1250b4f6e2573cde51c6f10d0c8a856f564c7275fe6fe6e2caf68b3c1e4 UID:585ce8ec-311a-4a7b-a35f-b6896fdd10fd NetNS:/var/run/netns/724ead4f-d61c-461e-90d7-f416d4e52b44 Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.612207850Z" level=info msg="Checking pod kube-system_coredns-6f6b679f8f-znkjh for CNI network kindnet (type=ptp)"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.614080403Z" level=info msg="Ran pod sandbox ddacf1250b4f6e2573cde51c6f10d0c8a856f564c7275fe6fe6e2caf68b3c1e4 with infra container: kube-system/coredns-6f6b679f8f-znkjh/POD" id=b84d5ae4-673f-4f86-aad6-29e502d074fa name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.614834825Z" level=info msg="Checking image status: registry.k8s.io/coredns/coredns:v1.11.1" id=b6b01982-8c86-41f6-bbc7-76329410fd9d name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.614966784Z" level=info msg="Image status: &ImageStatusResponse{Image:&Image{Id:2437cf762177702dec2dfe99a09c37427a15af6d9a57c456b65352667c223d93,RepoTags:[registry.k8s.io/coredns/coredns:v1.11.1],RepoDigests:[registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1 registry.k8s.io/coredns/coredns@sha256:ba9e70dbdf0ff8a77ea63451bb1241d08819471730fe7a35a218a8db2ef7890c],Size_:58812704,Uid:nil,Username:nonroot,Spec:nil,},Info:map[string]string{},}" id=b6b01982-8c86-41f6-bbc7-76329410fd9d name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.615434495Z" level=info msg="Checking image status: registry.k8s.io/coredns/coredns:v1.11.1" id=f16fc730-1c7a-47a0-a57d-246c0b877f92 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.615687246Z" level=info msg="Image status: &ImageStatusResponse{Image:&Image{Id:2437cf762177702dec2dfe99a09c37427a15af6d9a57c456b65352667c223d93,RepoTags:[registry.k8s.io/coredns/coredns:v1.11.1],RepoDigests:[registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1 registry.k8s.io/coredns/coredns@sha256:ba9e70dbdf0ff8a77ea63451bb1241d08819471730fe7a35a218a8db2ef7890c],Size_:58812704,Uid:nil,Username:nonroot,Spec:nil,},Info:map[string]string{},}" id=f16fc730-1c7a-47a0-a57d-246c0b877f92 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.617001921Z" level=info msg="Creating container: kube-system/coredns-6f6b679f8f-znkjh/coredns" id=ed5d9b4b-d2ab-4091-a363-b5106f48b2a6 name=/runtime.v1.RuntimeService/CreateContainer
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.617060838Z" level=warning msg="Allowed annotations are specified for workload []"
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.665917928Z" level=info msg="Created container 6c6c580d43cefcb7bc3794e3e8c5d173c3d9458e833ba5eaf710abec15a6e7a0: kube-system/coredns-6f6b679f8f-znkjh/coredns" id=ed5d9b4b-d2ab-4091-a363-b5106f48b2a6 name=/runtime.v1.RuntimeService/CreateContainer
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.666396223Z" level=info msg="Starting container: 6c6c580d43cefcb7bc3794e3e8c5d173c3d9458e833ba5eaf710abec15a6e7a0" id=96c408ea-2138-4e98-929a-e9038140dfd4 name=/runtime.v1.RuntimeService/StartContainer
Nov 04 17:23:33 minikube crio[2074]: time="2024-11-04 17:23:33.670084454Z" level=info msg="Started container" PID=2671 containerID=6c6c580d43cefcb7bc3794e3e8c5d173c3d9458e833ba5eaf710abec15a6e7a0 description=kube-system/coredns-6f6b679f8f-znkjh/coredns id=96c408ea-2138-4e98-929a-e9038140dfd4 name=/runtime.v1.RuntimeService/StartContainer sandboxID=ddacf1250b4f6e2573cde51c6f10d0c8a856f564c7275fe6fe6e2caf68b3c1e4
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.876250353Z" level=info msg="Running pod sandbox: auto-pause/env-inject-85b8f7f46d-sfqzv/POD" id=5b1dab21-bfec-4d15-9eb9-a63f783b1597 name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.876320479Z" level=warning msg="Allowed annotations are specified for workload []"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.877842446Z" level=info msg="Running pod sandbox: auto-pause/auto-pause-proxy-654bcfdc5d-srrb6/POD" id=ba36c598-8a0d-453f-9ec2-4efee08b62e1 name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.877899030Z" level=warning msg="Allowed annotations are specified for workload []"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.887135128Z" level=info msg="Got pod network &{Name:env-inject-85b8f7f46d-sfqzv Namespace:auto-pause ID:f6d24e2b70ed4def35d6177ca862794fd6aed5f2ba7e702b162155e5b32f0329 UID:7e4abead-8f32-4b16-ac9f-dbb1361a2817 NetNS:/var/run/netns/52eaf1e3-7675-4996-a372-1e185e0db4cc Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.887186087Z" level=info msg="Adding pod auto-pause_env-inject-85b8f7f46d-sfqzv to CNI network \"kindnet\" (type=ptp)"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.889684435Z" level=info msg="Got pod network &{Name:auto-pause-proxy-654bcfdc5d-srrb6 Namespace:auto-pause ID:2e8735264585659b73a3b884a258b1b95975309a82f72382723e9db7709e26e1 UID:7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2 NetNS:/var/run/netns/0c64068f-7e55-4ada-89bd-2d8c2e3d36a5 Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.889712102Z" level=info msg="Adding pod auto-pause_auto-pause-proxy-654bcfdc5d-srrb6 to CNI network \"kindnet\" (type=ptp)"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.894065296Z" level=info msg="Got pod network &{Name:env-inject-85b8f7f46d-sfqzv Namespace:auto-pause ID:f6d24e2b70ed4def35d6177ca862794fd6aed5f2ba7e702b162155e5b32f0329 UID:7e4abead-8f32-4b16-ac9f-dbb1361a2817 NetNS:/var/run/netns/52eaf1e3-7675-4996-a372-1e185e0db4cc Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.894145046Z" level=info msg="Checking pod auto-pause_env-inject-85b8f7f46d-sfqzv for CNI network kindnet (type=ptp)"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.895627472Z" level=info msg="Got pod network &{Name:auto-pause-proxy-654bcfdc5d-srrb6 Namespace:auto-pause ID:2e8735264585659b73a3b884a258b1b95975309a82f72382723e9db7709e26e1 UID:7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2 NetNS:/var/run/netns/0c64068f-7e55-4ada-89bd-2d8c2e3d36a5 Networks:[] RuntimeConfig:map[kindnet:{IP: MAC: PortMappings:[] Bandwidth:<nil> IpRanges:[]}] Aliases:map[]}"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.895657139Z" level=info msg="Ran pod sandbox f6d24e2b70ed4def35d6177ca862794fd6aed5f2ba7e702b162155e5b32f0329 with infra container: auto-pause/env-inject-85b8f7f46d-sfqzv/POD" id=5b1dab21-bfec-4d15-9eb9-a63f783b1597 name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.895707889Z" level=info msg="Checking pod auto-pause_auto-pause-proxy-654bcfdc5d-srrb6 for CNI network kindnet (type=ptp)"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.895768181Z" level=info msg="Ensuring kubelet hostport chains"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.896474352Z" level=info msg="Checking image status: gcr.io/k8s-minikube/auto-pause-hook:v0.0.5@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522" id=ab634981-bb6f-456e-a7eb-cd4e73efc815 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.896646311Z" level=info msg="Image gcr.io/k8s-minikube/auto-pause-hook:v0.0.5@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522 not found" id=ab634981-bb6f-456e-a7eb-cd4e73efc815 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.897027272Z" level=info msg="Pulling image: gcr.io/k8s-minikube/auto-pause-hook:v0.0.5@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522" id=becaf6aa-d452-40a6-b6bc-5d3077894459 name=/runtime.v1.ImageService/PullImage
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.900666794Z" level=info msg="Opened local port tcp:32443"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.902201470Z" level=info msg="Restoring iptables rules: *nat\n:KUBE-HP-KTUK2IQMMUOMFVI4 - [0:0]\n:KUBE-HOSTPORTS - [0:0]\n-I KUBE-HOSTPORTS -m comment --comment \"k8s_auto-pause-proxy-654bcfdc5d-srrb6_auto-pause_7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2_0_ hostport 32443\" -m tcp -p tcp --dport 32443 -j KUBE-HP-KTUK2IQMMUOMFVI4\n-A KUBE-HP-KTUK2IQMMUOMFVI4 -m comment --comment \"k8s_auto-pause-proxy-654bcfdc5d-srrb6_auto-pause_7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2_0_ hostport 32443\" -s 10.244.0.4 -j KUBE-MARK-MASQ\n-A KUBE-HP-KTUK2IQMMUOMFVI4 -m comment --comment \"k8s_auto-pause-proxy-654bcfdc5d-srrb6_auto-pause_7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2_0_ hostport 32443\" -m tcp -p tcp -j DNAT --to-destination=10.244.0.4:6443\nCOMMIT\n"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.903150559Z" level=info msg="Starting to delete udp conntrack entries: [], isIPv6 - false"
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.904506693Z" level=info msg="Ran pod sandbox 2e8735264585659b73a3b884a258b1b95975309a82f72382723e9db7709e26e1 with infra container: auto-pause/auto-pause-proxy-654bcfdc5d-srrb6/POD" id=ba36c598-8a0d-453f-9ec2-4efee08b62e1 name=/runtime.v1.RuntimeService/RunPodSandbox
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.905106113Z" level=info msg="Checking image status: haproxy:2.3.5-alpine" id=22fc5318-0fc2-4084-a8b8-2d5164df4f58 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.941921838Z" level=info msg="Trying to access \"gcr.io/k8s-minikube/auto-pause-hook@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522\""
Nov 04 17:23:38 minikube crio[2074]: time="2024-11-04 17:23:38.967863247Z" level=info msg="Checking image status: haproxy:2.3.5-alpine" id=1e941795-1c41-44b2-bee3-f5a790bc351c name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:39 minikube crio[2074]: time="2024-11-04 17:23:39.713505220Z" level=info msg="Trying to access \"gcr.io/k8s-minikube/auto-pause-hook@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522\""
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.030097919Z" level=info msg="Pulled image: gcr.io/k8s-minikube/auto-pause-hook@sha256:445d3fee4957faa056ec90fb753cc7e6efabd76ba4d1798a50cbf90c909687a3" id=becaf6aa-d452-40a6-b6bc-5d3077894459 name=/runtime.v1.ImageService/PullImage
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.030741090Z" level=info msg="Checking image status: gcr.io/k8s-minikube/auto-pause-hook:v0.0.5@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522" id=7415d6fb-64e2-4229-8e0c-081a6518ba3f name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.031523761Z" level=info msg="Image status: &ImageStatusResponse{Image:&Image{Id:37295df939343138675a22bb73b8f5640d7f684163f318abfbab6809f72adadd,RepoTags:[],RepoDigests:[gcr.io/k8s-minikube/auto-pause-hook@sha256:445d3fee4957faa056ec90fb753cc7e6efabd76ba4d1798a50cbf90c909687a3 gcr.io/k8s-minikube/auto-pause-hook@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522],Size_:44959432,Uid:nil,Username:,Spec:nil,},Info:map[string]string{},}" id=7415d6fb-64e2-4229-8e0c-081a6518ba3f name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.032596184Z" level=info msg="Checking image status: gcr.io/k8s-minikube/auto-pause-hook:v0.0.5@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522" id=3a91991d-ca82-4349-bfdb-02d6367b0dc7 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.033611690Z" level=info msg="Image status: &ImageStatusResponse{Image:&Image{Id:37295df939343138675a22bb73b8f5640d7f684163f318abfbab6809f72adadd,RepoTags:[],RepoDigests:[gcr.io/k8s-minikube/auto-pause-hook@sha256:445d3fee4957faa056ec90fb753cc7e6efabd76ba4d1798a50cbf90c909687a3 gcr.io/k8s-minikube/auto-pause-hook@sha256:d613ed2c891882b602b5aca668e92d4606a1b3832d96750ab25804de15929522],Size_:44959432,Uid:nil,Username:,Spec:nil,},Info:map[string]string{},}" id=3a91991d-ca82-4349-bfdb-02d6367b0dc7 name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.034609113Z" level=info msg="Creating container: auto-pause/env-inject-85b8f7f46d-sfqzv/webhook" id=8d7d8871-686e-4f20-9df8-a77a1fbd0017 name=/runtime.v1.RuntimeService/CreateContainer
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.034667739Z" level=warning msg="Allowed annotations are specified for workload []"
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.041403113Z" level=warning msg="Failed to open /etc/passwd: open /var/lib/containers/storage/overlay/08b7bc70bf6f77d01c66a151416804f15f6ee209e230a780895e1b07f8e2adb3/merged/etc/passwd: no such file or directory"
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.041430155Z" level=warning msg="Failed to open /etc/group: open /var/lib/containers/storage/overlay/08b7bc70bf6f77d01c66a151416804f15f6ee209e230a780895e1b07f8e2adb3/merged/etc/group: no such file or directory"
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.067465147Z" level=info msg="Created container 8094722c968362bd03b68d5950ac57395ca579e117ea85754ce950a6eea2a4f6: auto-pause/env-inject-85b8f7f46d-sfqzv/webhook" id=8d7d8871-686e-4f20-9df8-a77a1fbd0017 name=/runtime.v1.RuntimeService/CreateContainer
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.068215860Z" level=info msg="Starting container: 8094722c968362bd03b68d5950ac57395ca579e117ea85754ce950a6eea2a4f6" id=c1edd36e-8d2f-4293-97c9-e37d9def57db name=/runtime.v1.RuntimeService/StartContainer
Nov 04 17:23:42 minikube crio[2074]: time="2024-11-04 17:23:42.071879841Z" level=info msg="Started container" PID=2777 containerID=8094722c968362bd03b68d5950ac57395ca579e117ea85754ce950a6eea2a4f6 description=auto-pause/env-inject-85b8f7f46d-sfqzv/webhook id=c1edd36e-8d2f-4293-97c9-e37d9def57db name=/runtime.v1.RuntimeService/StartContainer sandboxID=f6d24e2b70ed4def35d6177ca862794fd6aed5f2ba7e702b162155e5b32f0329
Nov 04 17:23:53 minikube crio[2074]: time="2024-11-04 17:23:53.591111280Z" level=info msg="Checking image status: haproxy:2.3.5-alpine" id=7fb2d47c-447a-4cd0-8203-493b673cf82b name=/runtime.v1.ImageService/ImageStatus
Nov 04 17:24:06 minikube crio[2074]: time="2024-11-04 17:24:06.590399180Z" level=info msg="Checking image status: haproxy:2.3.5-alpine" id=f581dd82-978f-4bfb-bfb7-7eae14b2f019 name=/runtime.v1.ImageService/ImageStatus


==> container status <==
CONTAINER           IMAGE                                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8094722c96836       gcr.io/k8s-minikube/auto-pause-hook@sha256:445d3fee4957faa056ec90fb753cc7e6efabd76ba4d1798a50cbf90c909687a3   15 minutes ago      Running             webhook                   0                   f6d24e2b70ed4       env-inject-85b8f7f46d-sfqzv
6c6c580d43cef       2437cf762177702dec2dfe99a09c37427a15af6d9a57c456b65352667c223d93                                              15 minutes ago      Running             coredns                   0                   ddacf1250b4f6       coredns-6f6b679f8f-znkjh
ad950f8d72c76       docker.io/kindest/kindnetd@sha256:4d39335073da9a0b82be8e01028f0aa75aff16caff2e2d8889d0effd579a6f64            15 minutes ago      Running             kindnet-cni               0                   708ba5e630089       kindnet-gmrjn
ecd55df8b9688       ba04bb24b95753201135cbc420b233c1b0b9fa2e1fd21d28319c348c33fbcde6                                              15 minutes ago      Running             storage-provisioner       0                   6d50d01d694ed       storage-provisioner
b85943747edf2       71d55d66fd4eec8986225089a135fadd96bc6624d987096808772ce1e1924d89                                              16 minutes ago      Running             kube-proxy                0                   4c6f7a456b5ea       kube-proxy-xj8wm
e83b982e8661d       cd0f0ae0ec9e0cdc092079156c122bf034ba3f24d31c1b1dd1b52a42ecf9b388                                              16 minutes ago      Running             kube-apiserver            0                   6be0c64796923       kube-apiserver-minikube
47113c87d8cc1       27e3830e1402783674d8b594038967deea9d51f0d91b34c93c8f39d2f68af7da                                              16 minutes ago      Running             etcd                      0                   2e24244b7a081       etcd-minikube
229cbf17717a1       fbbbd428abb4dae52ab3018797d00d5840a739f0cc5697b662791831a60b0adb                                              16 minutes ago      Running             kube-scheduler            0                   c9f6b752a65d0       kube-scheduler-minikube
b139caf73ca80       fcb0683e6bdbd083710cf2d6fd7eb699c77fe4994c38a5c82d059e2e3cb4c2fd                                              16 minutes ago      Running             kube-controller-manager   0                   ba4753157937c       kube-controller-manager-minikube


==> coredns [6c6c580d43cefcb7bc3794e3e8c5d173c3d9458e833ba5eaf710abec15a6e7a0] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2


==> describe nodes <==
command /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" failed with error: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
Unable to connect to the server: net/http: TLS handshake timeout


==> dmesg <==
[Nov 4 13:11] KASLR disabled due to lack of seed
[  +0.000000] ACPI PPTT: No PPTT table found, CPU and cache topology may be inaccurate
[  +0.000033] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.366940] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.251188] ln: 
[  +0.000003] failed to create symbolic link '/var/log'
[  +0.000004] : Read-only file system

[  +0.481262] 55-scsi-sg3_id.rules[663]: WARNING: SCSI device vda has no device ID, consider changing .SCSI_ID_SERIAL_SRC in 00-scsi-sg3_config.rules
[Nov 4 13:17] hrtimer: interrupt took 3547193 ns


==> etcd [47113c87d8cc1a90dd83a819221047b181c172117db8768a082cee8042a092c6] <==
{"level":"warn","ts":"2024-11-04T17:22:13.312879Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-04T17:22:13.313052Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-11-04T17:22:13.313116Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-04T17:22:13.313123Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-04T17:22:13.313138Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-04T17:22:13.313564Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-11-04T17:22:13.313687Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-04T17:22:13.315500Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.645968ms"}
{"level":"info","ts":"2024-11-04T17:22:13.318165Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-11-04T17:22:13.318282Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-11-04T17:22:13.318331Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-11-04T17:22:13.318366Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-11-04T17:22:13.318383Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-11-04T17:22:13.318418Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-11-04T17:22:13.319729Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-04T17:22:13.322777Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-11-04T17:22:13.323134Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-04T17:22:13.323936Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-11-04T17:22:13.324073Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T17:22:13.324097Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T17:22:13.324102Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T17:22:13.324092Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-04T17:22:13.324285Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T17:22:13.324571Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-11-04T17:22:13.324655Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-04T17:22:13.325176Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-04T17:22:13.325401Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-04T17:22:13.325415Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-04T17:22:13.325697Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-04T17:22:13.325731Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-04T17:22:13.419373Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-11-04T17:22:13.419408Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-11-04T17:22:13.419427Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-11-04T17:22:13.419440Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-11-04T17:22:13.419443Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-04T17:22:13.419465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-11-04T17:22:13.419470Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-04T17:22:13.419943Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-04T17:22:13.419948Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-04T17:22:13.419991Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T17:22:13.420055Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-04T17:22:13.420283Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T17:22:13.420340Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T17:22:13.420370Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T17:22:13.420523Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-04T17:22:13.420551Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-04T17:22:13.420621Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T17:22:13.420747Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T17:22:13.421200Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-04T17:22:13.421303Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 17:39:13 up  4:27,  0 users,  load average: 1.29, 1.00, 0.96
Linux minikube 6.11.3-200.fc40.aarch64 #1 SMP PREEMPT_DYNAMIC Thu Oct 10 22:53:48 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kindnet [ad950f8d72c7601a2df7e792519083efaeaf257f1f770f490779f9bdda640527] <==
I1104 17:23:20.584287       1 main.go:109] connected to apiserver: https://10.96.0.1:443
I1104 17:23:20.584588       1 main.go:139] hostIP = 192.168.49.2
podIP = 192.168.49.2
I1104 17:23:20.584709       1 main.go:148] setting mtu 1500 for CNI 
I1104 17:23:20.584721       1 main.go:178] kindnetd IP family: "ipv4"
I1104 17:23:20.584730       1 main.go:182] noMask IPv4 subnets: [10.244.0.0/16]
I1104 17:23:20.789232       1 controller.go:334] Starting controller kube-network-policies
I1104 17:23:20.789254       1 controller.go:338] Waiting for informer caches to sync
I1104 17:23:20.789258       1 shared_informer.go:313] Waiting for caches to sync for kube-network-policies
I1104 17:23:20.990136       1 shared_informer.go:320] Caches are synced for kube-network-policies
I1104 17:23:20.990160       1 metrics.go:61] Registering metrics
I1104 17:23:20.990206       1 controller.go:374] Syncing nftables rules
I1104 17:23:30.798638       1 main.go:295] Handling node with IPs: map[192.168.49.2:{}]
I1104 17:23:30.798682       1 main.go:299] handling current node
I1104 17:23:40.789313       1 main.go:295] Handling node with IPs: map[192.168.49.2:{}]
I1104 17:23:40.789381       1 main.go:299] handling current node
I1104 17:23:50.790654       1 main.go:295] Handling node with IPs: map[192.168.49.2:{}]
I1104 17:23:50.790688       1 main.go:299] handling current node
I1104 17:24:00.797574       1 main.go:295] Handling node with IPs: map[192.168.49.2:{}]
I1104 17:24:00.797609       1 main.go:299] handling current node
I1104 17:24:10.795195       1 main.go:295] Handling node with IPs: map[192.168.49.2:{}]
I1104 17:24:10.795231       1 main.go:299] handling current node


==> kube-apiserver [e83b982e8661d162a8ec48c4910eccdac36770ef79e96bf67a5bf3f3e4674cab] <==
I1104 17:22:14.534143       1 aggregator.go:169] waiting for initial CRD sync...
I1104 17:22:14.534148       1 local_available_controller.go:156] Starting LocalAvailability controller
I1104 17:22:14.534150       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1104 17:22:14.534161       1 controller.go:78] Starting OpenAPI AggregationController
I1104 17:22:14.534021       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1104 17:22:14.534173       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1104 17:22:14.534192       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1104 17:22:14.534249       1 controller.go:119] Starting legacy_token_tracking_controller
I1104 17:22:14.534254       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1104 17:22:14.534265       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1104 17:22:14.534310       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1104 17:22:14.534329       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1104 17:22:14.534335       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1104 17:22:14.534346       1 controller.go:142] Starting OpenAPI controller
I1104 17:22:14.534356       1 controller.go:90] Starting OpenAPI V3 controller
I1104 17:22:14.534362       1 naming_controller.go:294] Starting NamingConditionController
I1104 17:22:14.534369       1 establishing_controller.go:81] Starting EstablishingController
I1104 17:22:14.534379       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1104 17:22:14.534384       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1104 17:22:14.534391       1 crd_finalizer.go:269] Starting CRDFinalizer
I1104 17:22:14.541842       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1104 17:22:14.541867       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1104 17:22:14.534329       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1104 17:22:14.634692       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1104 17:22:14.634714       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1104 17:22:14.634738       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1104 17:22:14.634742       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1104 17:22:14.634743       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1104 17:22:14.634754       1 cache.go:39] Caches are synced for LocalAvailability controller
I1104 17:22:14.635009       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1104 17:22:14.635067       1 shared_informer.go:320] Caches are synced for configmaps
I1104 17:22:14.642853       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1104 17:22:14.642878       1 aggregator.go:171] initial CRD sync complete...
I1104 17:22:14.642882       1 autoregister_controller.go:144] Starting autoregister controller
I1104 17:22:14.642887       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1104 17:22:14.642889       1 cache.go:39] Caches are synced for autoregister controller
E1104 17:22:14.646783       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1104 17:22:14.657265       1 shared_informer.go:320] Caches are synced for node_authorizer
I1104 17:22:14.661518       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1104 17:22:14.661540       1 policy_source.go:224] refreshing policies
E1104 17:22:14.688399       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1104 17:22:14.735809       1 controller.go:615] quota admission added evaluator for: namespaces
I1104 17:22:14.848588       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1104 17:22:15.539438       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1104 17:22:15.541944       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1104 17:22:15.541959       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1104 17:22:15.808769       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1104 17:22:15.829192       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1104 17:22:15.944907       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1104 17:22:15.948560       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1104 17:22:15.949280       1 controller.go:615] quota admission added evaluator for: endpoints
I1104 17:22:15.952910       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1104 17:22:16.551428       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1104 17:22:16.847154       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1104 17:22:16.852552       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1104 17:22:16.858148       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1104 17:22:21.953670       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1104 17:22:21.953670       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1104 17:22:22.303824       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1104 17:23:18.528221       1 alloc.go:330] "allocated clusterIPs" service="auto-pause/webhook" clusterIPs={"IPv4":"10.109.151.174"}


==> kube-controller-manager [b139caf73ca80c6a98df233ff58336b1d9d1195fb4e61c4f268a9c300153f979] <==
I1104 17:22:21.283182       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:22:21.284674       1 shared_informer.go:320] Caches are synced for job
I1104 17:22:21.292724       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1104 17:22:21.297476       1 shared_informer.go:320] Caches are synced for ReplicationController
I1104 17:22:21.300141       1 shared_informer.go:320] Caches are synced for service account
I1104 17:22:21.301395       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1104 17:22:21.301475       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1104 17:22:21.301482       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1104 17:22:21.302202       1 shared_informer.go:320] Caches are synced for PV protection
I1104 17:22:21.302244       1 shared_informer.go:320] Caches are synced for ephemeral
I1104 17:22:21.302254       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1104 17:22:21.302290       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1104 17:22:21.303918       1 shared_informer.go:320] Caches are synced for TTL
I1104 17:22:21.304108       1 shared_informer.go:320] Caches are synced for PVC protection
I1104 17:22:21.304522       1 shared_informer.go:320] Caches are synced for crt configmap
I1104 17:22:21.305904       1 shared_informer.go:320] Caches are synced for deployment
I1104 17:22:21.308555       1 shared_informer.go:320] Caches are synced for disruption
I1104 17:22:21.309859       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1104 17:22:21.403910       1 shared_informer.go:320] Caches are synced for attach detach
I1104 17:22:21.462853       1 shared_informer.go:320] Caches are synced for cronjob
I1104 17:22:21.500576       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:22:21.514023       1 shared_informer.go:320] Caches are synced for resource quota
I1104 17:22:21.554314       1 shared_informer.go:320] Caches are synced for resource quota
I1104 17:22:21.557604       1 shared_informer.go:320] Caches are synced for HPA
I1104 17:22:21.926880       1 shared_informer.go:320] Caches are synced for garbage collector
I1104 17:22:22.004016       1 shared_informer.go:320] Caches are synced for garbage collector
I1104 17:22:22.004039       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1104 17:22:22.205811       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:22:22.410175       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="104.348971ms"
I1104 17:22:22.413920       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="3.694439ms"
I1104 17:22:22.413983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="36.875¬µs"
I1104 17:23:05.256309       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:05.267198       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:05.272949       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="226.376¬µs"
I1104 17:23:05.284965       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="33.042¬µs"
I1104 17:23:06.522728       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:06.523142       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1104 17:23:07.865298       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:07.880914       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:11.523618       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1104 17:23:18.480741       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="13.041288ms"
I1104 17:23:18.483681       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="2.821351ms"
I1104 17:23:18.483720       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="13.083¬µs"
I1104 17:23:18.524582       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="6.469332ms"
I1104 17:23:18.527512       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="2.888518ms"
I1104 17:23:18.527560       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="19.459¬µs"
I1104 17:23:33.967530       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="35.167¬µs"
I1104 17:23:33.980679       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.166942ms"
I1104 17:23:33.981059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="77.584¬µs"
I1104 17:23:38.565135       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:38.570872       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1104 17:23:38.574600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="51.583¬µs"
I1104 17:23:38.574705       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="11.375¬µs"
I1104 17:23:38.582005       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="32¬µs"
I1104 17:23:38.586504       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="15.459¬µs"
I1104 17:23:38.973359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/auto-pause-proxy-654bcfdc5d" duration="31.333¬µs"
I1104 17:23:41.536966       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1104 17:23:43.008034       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="3.427063ms"
I1104 17:23:43.008182       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="auto-pause/env-inject-85b8f7f46d" duration="36.209¬µs"
I1104 17:23:48.583215       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [b85943747edf25430c154da006e745761d67c5b0cf4846425388691168fa3621] <==
I1104 17:22:22.970933       1 server_linux.go:66] "Using iptables proxy"
I1104 17:22:23.034587       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1104 17:22:23.034629       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1104 17:22:23.046006       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1104 17:22:23.046044       1 server_linux.go:169] "Using iptables Proxier"
I1104 17:22:23.047023       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1104 17:22:23.047262       1 server.go:483] "Version info" version="v1.31.0"
I1104 17:22:23.047274       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 17:22:23.048107       1 config.go:197] "Starting service config controller"
I1104 17:22:23.048140       1 shared_informer.go:313] Waiting for caches to sync for service config
I1104 17:22:23.048410       1 config.go:104] "Starting endpoint slice config controller"
I1104 17:22:23.048438       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1104 17:22:23.048833       1 config.go:326] "Starting node config controller"
I1104 17:22:23.048915       1 shared_informer.go:313] Waiting for caches to sync for node config
I1104 17:22:23.148499       1 shared_informer.go:320] Caches are synced for service config
I1104 17:22:23.148527       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1104 17:22:23.149028       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [229cbf17717a190ded570c58b2580024b2614aaafdc87b5e1234a72c12a84454] <==
I1104 17:22:14.247701       1 serving.go:386] Generated self-signed cert in-memory
W1104 17:22:14.720866       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1104 17:22:14.720886       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1104 17:22:14.720894       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1104 17:22:14.720897       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1104 17:22:14.727207       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1104 17:22:14.727251       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 17:22:14.728356       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1104 17:22:14.728378       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1104 17:22:14.728355       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1104 17:22:14.728432       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W1104 17:22:14.729262       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1104 17:22:14.729291       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1104 17:22:14.729686       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1104 17:22:14.729729       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.729768       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1104 17:22:14.729789       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.729891       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1104 17:22:14.729904       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1104 17:22:14.729906       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1104 17:22:14.729913       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.729951       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1104 17:22:14.729964       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.729995       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1104 17:22:14.730004       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730088       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1104 17:22:14.730107       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730170       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1104 17:22:14.730185       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1104 17:22:14.730184       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1104 17:22:14.730191       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730237       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1104 17:22:14.730263       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1104 17:22:14.730270       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1104 17:22:14.730270       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730307       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1104 17:22:14.730315       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730377       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1104 17:22:14.730389       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 17:22:14.730432       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1104 17:22:14.730469       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1104 17:22:15.663300       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1104 17:22:15.663332       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 17:22:15.674319       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1104 17:22:15.674347       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 17:22:15.813769       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1104 17:22:15.813803       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1104 17:22:17.928994       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Nov 04 17:23:05 minikube kubelet[1423]: E1104 17:23:05.591405    1423 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-6f6b679f8f-znkjh_kube-system_585ce8ec-311a-4a7b-a35f-b6896fdd10fd_0(499a1bac30e94091bc70a287290ecafed2411460b05256e567508cd53decf48a): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh"
Nov 04 17:23:05 minikube kubelet[1423]: E1104 17:23:05.591439    1423 kuberuntime_manager.go:1168] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-6f6b679f8f-znkjh_kube-system_585ce8ec-311a-4a7b-a35f-b6896fdd10fd_0(499a1bac30e94091bc70a287290ecafed2411460b05256e567508cd53decf48a): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh"
Nov 04 17:23:05 minikube kubelet[1423]: E1104 17:23:05.591500    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-6f6b679f8f-znkjh_kube-system(585ce8ec-311a-4a7b-a35f-b6896fdd10fd)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-6f6b679f8f-znkjh_kube-system(585ce8ec-311a-4a7b-a35f-b6896fdd10fd)\\\": rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-6f6b679f8f-znkjh_kube-system_585ce8ec-311a-4a7b-a35f-b6896fdd10fd_0(499a1bac30e94091bc70a287290ecafed2411460b05256e567508cd53decf48a): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?\"" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:06 minikube kubelet[1423]: E1104 17:23:06.599815    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730740986599542019,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:125206,},InodesUsed:&UInt64Value{Value:57,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:06 minikube kubelet[1423]: E1104 17:23:06.599851    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730740986599542019,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:125206,},InodesUsed:&UInt64Value{Value:57,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:06 minikube kubelet[1423]: E1104 17:23:06.610050    1423 kubelet.go:2901] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"
Nov 04 17:23:07 minikube kubelet[1423]: I1104 17:23:07.859954    1423 setters.go:600] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2024-11-04T17:23:07Z","lastTransitionTime":"2024-11-04T17:23:07Z","reason":"KubeletNotReady","message":"container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"}
Nov 04 17:23:11 minikube kubelet[1423]: E1104 17:23:11.611804    1423 kubelet.go:2901] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"
Nov 04 17:23:16 minikube kubelet[1423]: I1104 17:23:16.596757    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=58.596737489 podStartE2EDuration="58.596737489s" podCreationTimestamp="2024-11-04 17:22:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-11-04 17:23:05.887955421 +0000 UTC m=+49.357314927" watchObservedRunningTime="2024-11-04 17:23:16.596737489 +0000 UTC m=+60.066096996"
Nov 04 17:23:16 minikube kubelet[1423]: E1104 17:23:16.601324    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730740996601155016,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:125206,},InodesUsed:&UInt64Value{Value:57,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:16 minikube kubelet[1423]: E1104 17:23:16.601345    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730740996601155016,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:125206,},InodesUsed:&UInt64Value{Value:57,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:16 minikube kubelet[1423]: E1104 17:23:16.613170    1423 kubelet.go:2901] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"
Nov 04 17:23:17 minikube kubelet[1423]: E1104 17:23:17.589510    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:19 minikube kubelet[1423]: E1104 17:23:19.589309    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:20 minikube kubelet[1423]: I1104 17:23:20.928005    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kindnet-gmrjn" podStartSLOduration=2.492176746 podStartE2EDuration="59.927990209s" podCreationTimestamp="2024-11-04 17:22:21 +0000 UTC" firstStartedPulling="2024-11-04 17:22:22.889858857 +0000 UTC m=+6.359218322" lastFinishedPulling="2024-11-04 17:23:20.32567232 +0000 UTC m=+63.795031785" observedRunningTime="2024-11-04 17:23:20.927220412 +0000 UTC m=+64.396579919" watchObservedRunningTime="2024-11-04 17:23:20.927990209 +0000 UTC m=+64.397349674"
Nov 04 17:23:21 minikube kubelet[1423]: E1104 17:23:21.588633    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:21 minikube kubelet[1423]: E1104 17:23:21.614246    1423 kubelet.go:2901] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"
Nov 04 17:23:23 minikube kubelet[1423]: E1104 17:23:23.589792    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:25 minikube kubelet[1423]: E1104 17:23:25.589770    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:26 minikube kubelet[1423]: E1104 17:23:26.603442    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741006603171766,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:135398,},InodesUsed:&UInt64Value{Value:63,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:26 minikube kubelet[1423]: E1104 17:23:26.603526    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741006603171766,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:135398,},InodesUsed:&UInt64Value{Value:63,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:26 minikube kubelet[1423]: E1104 17:23:26.615031    1423 kubelet.go:2901] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?"
Nov 04 17:23:27 minikube kubelet[1423]: E1104 17:23:27.589136    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:29 minikube kubelet[1423]: E1104 17:23:29.589348    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:31 minikube kubelet[1423]: E1104 17:23:31.589404    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/cni/net.d/. Has your network provider started?" pod="kube-system/coredns-6f6b679f8f-znkjh" podUID="585ce8ec-311a-4a7b-a35f-b6896fdd10fd"
Nov 04 17:23:33 minikube kubelet[1423]: I1104 17:23:33.976254    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-znkjh" podStartSLOduration=71.976238617 podStartE2EDuration="1m11.976238617s" podCreationTimestamp="2024-11-04 17:22:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-11-04 17:23:33.968800696 +0000 UTC m=+77.438160161" watchObservedRunningTime="2024-11-04 17:23:33.976238617 +0000 UTC m=+77.445598082"
Nov 04 17:23:36 minikube kubelet[1423]: E1104 17:23:36.605087    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741016604826389,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:135398,},InodesUsed:&UInt64Value{Value:63,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:36 minikube kubelet[1423]: E1104 17:23:36.605124    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741016604826389,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:135398,},InodesUsed:&UInt64Value{Value:63,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:38 minikube kubelet[1423]: I1104 17:23:38.761072    1423 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h78ck\" (UniqueName: \"kubernetes.io/projected/7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2-kube-api-access-h78ck\") pod \"auto-pause-proxy-654bcfdc5d-srrb6\" (UID: \"7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2\") " pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6"
Nov 04 17:23:38 minikube kubelet[1423]: I1104 17:23:38.761117    1423 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hq95s\" (UniqueName: \"kubernetes.io/projected/7e4abead-8f32-4b16-ac9f-dbb1361a2817-kube-api-access-hq95s\") pod \"env-inject-85b8f7f46d-sfqzv\" (UID: \"7e4abead-8f32-4b16-ac9f-dbb1361a2817\") " pod="auto-pause/env-inject-85b8f7f46d-sfqzv"
Nov 04 17:23:38 minikube kubelet[1423]: I1104 17:23:38.761133    1423 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lua-script\" (UniqueName: \"kubernetes.io/host-path/7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2-lua-script\") pod \"auto-pause-proxy-654bcfdc5d-srrb6\" (UID: \"7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2\") " pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6"
Nov 04 17:23:38 minikube kubelet[1423]: I1104 17:23:38.761151    1423 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ha-cfg\" (UniqueName: \"kubernetes.io/host-path/7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2-ha-cfg\") pod \"auto-pause-proxy-654bcfdc5d-srrb6\" (UID: \"7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2\") " pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.905346    1423 log.go:32] "Get ImageStatus from image service failed" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.905389    1423 kuberuntime_image.go:90] "Failed to get image status" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.905550    1423 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:auto-pause,Image:haproxy:2.3.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:32443,ContainerPort:6443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ha-cfg,ReadOnly:true,MountPath:/usr/local/etc/haproxy/haproxy.cfg,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:lua-script,ReadOnly:false,MountPath:/etc/haproxy/unpause.lua,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-h78ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auto-pause-proxy-654bcfdc5d-srrb6_auto-pause(7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2): ImageInspectError: Failed to inspect image \"haproxy:2.3.5-alpine\": rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" logger="UnhandledError"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.906708    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auto-pause\" with ImageInspectError: \"Failed to inspect image \\\"haproxy:2.3.5-alpine\\\": rpc error: code = Unknown desc = short-name \\\"haproxy:2.3.5-alpine\\\" did not resolve to an alias and no unqualified-search registries are defined in \\\"/etc/containers/registries.conf\\\"\"" pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6" podUID="7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.968079    1423 log.go:32] "Get ImageStatus from image service failed" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.968109    1423 kuberuntime_image.go:90] "Failed to get image status" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.968173    1423 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:auto-pause,Image:haproxy:2.3.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:32443,ContainerPort:6443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ha-cfg,ReadOnly:true,MountPath:/usr/local/etc/haproxy/haproxy.cfg,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:lua-script,ReadOnly:false,MountPath:/etc/haproxy/unpause.lua,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-h78ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auto-pause-proxy-654bcfdc5d-srrb6_auto-pause(7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2): ImageInspectError: Failed to inspect image \"haproxy:2.3.5-alpine\": rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" logger="UnhandledError"
Nov 04 17:23:38 minikube kubelet[1423]: E1104 17:23:38.969338    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auto-pause\" with ImageInspectError: \"Failed to inspect image \\\"haproxy:2.3.5-alpine\\\": rpc error: code = Unknown desc = short-name \\\"haproxy:2.3.5-alpine\\\" did not resolve to an alias and no unqualified-search registries are defined in \\\"/etc/containers/registries.conf\\\"\"" pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6" podUID="7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2"
Nov 04 17:23:46 minikube kubelet[1423]: E1104 17:23:46.606224    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741026605988925,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:46 minikube kubelet[1423]: E1104 17:23:46.606307    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741026605988925,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:53 minikube kubelet[1423]: E1104 17:23:53.591618    1423 log.go:32] "Get ImageStatus from image service failed" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:53 minikube kubelet[1423]: E1104 17:23:53.591686    1423 kuberuntime_image.go:90] "Failed to get image status" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:23:53 minikube kubelet[1423]: E1104 17:23:53.591806    1423 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:auto-pause,Image:haproxy:2.3.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:32443,ContainerPort:6443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ha-cfg,ReadOnly:true,MountPath:/usr/local/etc/haproxy/haproxy.cfg,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:lua-script,ReadOnly:false,MountPath:/etc/haproxy/unpause.lua,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-h78ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auto-pause-proxy-654bcfdc5d-srrb6_auto-pause(7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2): ImageInspectError: Failed to inspect image \"haproxy:2.3.5-alpine\": rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" logger="UnhandledError"
Nov 04 17:23:53 minikube kubelet[1423]: E1104 17:23:53.593414    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auto-pause\" with ImageInspectError: \"Failed to inspect image \\\"haproxy:2.3.5-alpine\\\": rpc error: code = Unknown desc = short-name \\\"haproxy:2.3.5-alpine\\\" did not resolve to an alias and no unqualified-search registries are defined in \\\"/etc/containers/registries.conf\\\"\"" pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6" podUID="7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2"
Nov 04 17:23:56 minikube kubelet[1423]: E1104 17:23:56.607551    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741036607357254,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:23:56 minikube kubelet[1423]: E1104 17:23:56.607592    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741036607357254,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.590818    1423 log.go:32] "Get ImageStatus from image service failed" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.590863    1423 kuberuntime_image.go:90] "Failed to get image status" err="rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" image="haproxy:2.3.5-alpine"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.590937    1423 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:auto-pause,Image:haproxy:2.3.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:32443,ContainerPort:6443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ha-cfg,ReadOnly:true,MountPath:/usr/local/etc/haproxy/haproxy.cfg,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:lua-script,ReadOnly:false,MountPath:/etc/haproxy/unpause.lua,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-h78ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auto-pause-proxy-654bcfdc5d-srrb6_auto-pause(7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2): ImageInspectError: Failed to inspect image \"haproxy:2.3.5-alpine\": rpc error: code = Unknown desc = short-name \"haproxy:2.3.5-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"" logger="UnhandledError"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.592367    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auto-pause\" with ImageInspectError: \"Failed to inspect image \\\"haproxy:2.3.5-alpine\\\": rpc error: code = Unknown desc = short-name \\\"haproxy:2.3.5-alpine\\\" did not resolve to an alias and no unqualified-search registries are defined in \\\"/etc/containers/registries.conf\\\"\"" pod="auto-pause/auto-pause-proxy-654bcfdc5d-srrb6" podUID="7a4df4bd-2b6a-4c30-aec6-2dc7e09328e2"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.608521    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741046608332414,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:24:06 minikube kubelet[1423]: E1104 17:24:06.608549    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741046608332414,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:24:16 minikube kubelet[1423]: E1104 17:24:16.609352    1423 eviction_manager.go:257] "Eviction manager: failed to get HasDedicatedImageFs" err="missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741056609188990,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:24:16 minikube kubelet[1423]: E1104 17:24:16.609386    1423 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: missing image stats: &ImageFsInfoResponse{ImageFilesystems:[]*FilesystemUsage{&FilesystemUsage{Timestamp:1730741056609188990,FsId:&FilesystemIdentifier{Mountpoint:/var/lib/containers/storage/overlay-images,},UsedBytes:&UInt64Value{Value:146886,},InodesUsed:&UInt64Value{Value:69,},},},ContainerFilesystems:[]*FilesystemUsage{},}"
Nov 04 17:24:19 minikube systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Nov 04 17:24:19 minikube systemd[1]: kubelet.service: Deactivated successfully.
Nov 04 17:24:19 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 04 17:24:19 minikube systemd[1]: kubelet.service: Consumed 2.589s CPU time.


==> storage-provisioner [ecd55df8b96886a9689cdc60103d6b72f36b79b613fc3434a2e4c195c98aeb07] <==
I1104 17:23:05.647574       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1104 17:23:05.652297       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1104 17:23:05.652325       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1104 17:23:05.658330       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1104 17:23:05.658437       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_7c7f0b7c-d4c1-49a9-8677-f5d4e7819c58!
I1104 17:23:05.658968       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"93d419e3-14ef-4f89-a79e-203348cf7822", APIVersion:"v1", ResourceVersion:"433", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_7c7f0b7c-d4c1-49a9-8677-f5d4e7819c58 became leader
I1104 17:23:05.759506       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_7c7f0b7c-d4c1-49a9-8677-f5d4e7819c58!

